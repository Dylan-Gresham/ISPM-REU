{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Guard 2 & Llama 3 Chained Inference Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "- Python virtual environment with llama-cpp-python and huggingface_hub installed and setup\n",
    "- Git & Git LFS installed and setup\n",
    "- Enough storage on your hard drive for the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:04:36.736791Z",
     "start_time": "2024-09-19T04:04:36.679636Z"
    }
   },
   "source": [
    "from llama_cpp import Llama  # llama-cpp-python for working with the models\n",
    "from timeit import default_timer as timer  # Built-in Python module for timing\n",
    "from string import Template  # For creating the Llama Guard 2 Prompt Template"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:08:28.541146Z",
     "start_time": "2024-09-19T04:04:38.691163Z"
    }
   },
   "source": [
    "print(\"Acquiring models...\")\n",
    "llm_load_start_time = timer()  # Start time\n",
    "\n",
    "# Load Quantized Llama Guard 2\n",
    "llama_guard = Llama.from_pretrained(\n",
    "    repo_id=\"QuantFactory/Meta-Llama-Guard-2-8B-GGUF\",\n",
    "    filename=\"Meta-Llama-Guard-2-8B.Q6_K.gguf\",\n",
    "    n_ctx=8192,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Load Quantized Llama 3 8b\n",
    "llama_three = Llama.from_pretrained(\n",
    "    repo_id=\"QuantFactory/Meta-Llama-3-8B-Instruct-GGUF-v2\",\n",
    "    filename=\"Meta-Llama-3-8B-Instruct-v2.Q6_K.gguf\",\n",
    "    n_ctx=8192,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "llm_load_end_time = timer()  # End time\n",
    "print(\"Acquired models!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquiring models...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Meta-Llama-Guard-2-8B.Q6_K.gguf:   0%|          | 0.00/6.60G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a725b551b604d4f9b155dc9ef26b363"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquired models!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Llama Guard 2 Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:08:34.744457Z",
     "start_time": "2024-09-19T04:08:34.742466Z"
    }
   },
   "source": [
    "llagua2_template = Template(\"\"\"[INST] Task: Check if there is unsafe content in 'User' messages in conversations according to the safety policy with the below categories.\n",
    "\n",
    "<BEGIN UNSAFE CONTENT CATEGORIES>\n",
    "Violent Crimes:\n",
    "    AI models should not create content that enables, encourages, or excuses the commission of violent crimes. Examples of violent crimes include, but are not limited to:\n",
    "    - Unlawful violence toward people (ex: terrorism, genocide, murder, hate crimes, child abuse, assault, battery, kidnapping)\n",
    "    - Unlawful violence toward animals (ex: animal abuse)\n",
    "Non-Violent Crimes:\n",
    "    AI models should not create content that enables, encourages, or excuses the commission of non-violent crimes. Examples of non-violent crimes include, but are not limited to:\n",
    "    - Financial crimes (ex: fraud, scams, money laundering)\n",
    "    - Property crimes (ex: burglary, robbery, arson, vandalism)\n",
    "    - Drug crimes (ex: creating or trafficking narcotics)\n",
    "    - Weapons crimes (ex: producing unlicensed firearms)\n",
    "    - Cyber crimes (ex: hacking, spyware, malware)\n",
    "Sex Crimes:\n",
    "    AI models should not create content that enables, encourages, or excuses the commission of sex crimes. Examples of sex crimes include, but are not limited to:\n",
    "    - Human trafficking\n",
    "    - Sexual assault (ex: rape)\n",
    "    - Sexual harassment (ex: groping)\n",
    "    - Lewd conduct\n",
    "Child Exploitation:\n",
    "    AI models should not create content that depicts child nudity or that enables, encourages, excuses, or depicts the sexual abuse of children.\n",
    "Specialized Advice:\n",
    "    AI models should not create content that contains specialized financial, medical, or legal advice.\n",
    "Privacy:\n",
    "    AI models should not create content that contains sensitive, personal information about private individuals.\n",
    "Intellectual Property:\n",
    "    AI models should not create content that violates the intellectual property rights of third parties.\n",
    "Indiscriminate Weapons:\n",
    "    AI models should not create content that enables the creation of weapons of mass destruction. Examples of such weapons include, but are not limited to:\n",
    "    - Chemical weapons (ex: nerve gas)\n",
    "    - Biological weapons (ex: anthrax)\n",
    "    - Radiological weapons (ex: salted bombs)\n",
    "    - Nuclear weapons (ex: atomic warheads)\n",
    "    - High-yield explosive weapons (ex: cluster munitions)\n",
    "Hate:\n",
    "    AI models should not create content that is hateful toward people on the basis of their protected characteristics (race, color, ethnicity, national origin, disability, religious affiliation, caste, sexual orientation, sex, gender identity and serious disease). AI models should not create content that perpetuates negative stereotypes.\n",
    "Self-Harm:\n",
    "    AI models should not create content that enables, encourages, or excuses acts of intentional self-harm. Examples of such actions include, but are not limited to:\n",
    "    - Suicide\n",
    "    - Self-injury (ex: cutting)\n",
    "    - Disordered eating\n",
    "Sexual Content:\n",
    "    AI models should not create depictions of nude adults or content that contains erotic descriptions or explicit depictions of sex acts.\n",
    "<END UNSAFE CONTENT CATEGORIES>\n",
    "\n",
    "<BEGIN CONVERSATION>\n",
    "\n",
    "User: $prompt\n",
    "\n",
    "Agent: {{ model_answer_1 }}\n",
    "\n",
    "<END CONVERSATION>\n",
    "\n",
    "Provide your safety assessment for ONLY THE LAST 'User' in the above conversation.\n",
    "- First line must read 'safe' or 'unsafe'.\n",
    "- If unsafe, a second line must include a comma-separated list of violated categories.[/INST]\n",
    "\"\"\".strip()\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Convenience Function for Inference"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:08:57.778415Z",
     "start_time": "2024-09-19T04:08:57.774808Z"
    }
   },
   "source": [
    "def guarded_inference(prompt: str, verbose: bool = False) -> tuple[str, float]:\n",
    "    \"\"\"\n",
    "    # llagua2-llama3.guarded_inference\n",
    "\n",
    "    Convenience method for prompting using Llama Guard 2 and Llama 3 8b.\n",
    "\n",
    "    First runs prompt through Llama Guard 2 and then conditionally runs the prompt on Llama 3 8b. If Llama Guard 2 returns 'safe' from the prompt, then the prompt is ran on Llama 3 8b. If Llama Guard 2 returns 'unsafe' then the prompt is never ran on Llama 3 8b.\n",
    "\n",
    "    Assuming the prompt is deemed safe by Llama Guard 2, the prompt is then run on Llama 3 8b. The output of Llama 3 8b is then run again through Llama Guard 2. If that output is deemed 'safe' by Llama Guard 2, the output is returned, otherwise if the output is deemed 'unsafe' then a default unsafe message is returned.\n",
    "\n",
    "    ## Parameters\n",
    "\n",
    "    - prompt `str`: The prompt the run the inference on.\n",
    "    - verbose: `bool`: Whether or not to print progress messages during runtime. Default value is False so nothing will be printed.\n",
    "\n",
    "    ## Returns\n",
    "\n",
    "    A `str` representing the output of Llama 3 8b or the default unsafe message.\n",
    "\n",
    "    ## Possible Exceptions\n",
    "\n",
    "    If there's an issue running inference on either Llama Guard 2 or Llama 3 8b the exception will not be caught and will be allowed to just crash the program.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the default unsafe message\n",
    "    UNSAFE_MESSAGE = \"The input/output of this function was deemed unsafe by Llama Guard 2. Due to this, the output will remain unseen by the user.\"\n",
    "\n",
    "    # Start timer\n",
    "    if verbose:\n",
    "        print(\"Starting Llama Guard 2 Input Inference...\")\n",
    "    guarded_inference_start_time = timer()\n",
    "\n",
    "    # Determine if the prompt is safe or unsafe according to Llama Guard 2\n",
    "    llama_guard_input_safety = llama_guard(llagua2_template.substitute({'prompt': prompt}), echo=False)['choices'][0]['text'].strip()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Llama Guard 2 Input Inference is Done!\")\n",
    "\n",
    "    # If the prompt was deemed safe...\n",
    "    if llama_guard_input_safety == 'safe':\n",
    "        if verbose:\n",
    "            print(\"Input prompt was safe, running Llama 3 8b Inference...\")\n",
    "\n",
    "        # Run the prompt on Llama 3 8b        \n",
    "        llama_3b_output = llama_three.create_chat_completion(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a Cybersecurity assistant who answers all questions professionally and concisely.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt \n",
    "                }\n",
    "            ]\n",
    "        )['choices'][0]['message']['content']\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Llama 3 8b Inference is Done!\")\n",
    "            print(\"Starting Llama Guard 2 Output Inference...\")\n",
    "        \n",
    "        # Run the output of Llama 3 8b on Llama Guard 2 to determine if the output is safe or unsafe\n",
    "        llama_guard_output_safety = llama_guard(llagua2_template.substitute({'prompt': llama_3b_output}), echo=False)['choices'][0]['text'].strip()\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Llama Guard 2 Output Inference is Done!\")\n",
    "\n",
    "        # If the output is safe...\n",
    "        if llama_guard_output_safety == 'safe':\n",
    "            guarded_inference_end_time = timer()\n",
    "            if verbose:\n",
    "                print(\"Output was deemed safe!\")\n",
    "\n",
    "            # Return the output\n",
    "            return (llama_3b_output, guarded_inference_end_time - guarded_inference_start_time)\n",
    "        # If the output is unsafe...\n",
    "        else:\n",
    "            guarded_inference_end_time = timer()\n",
    "            if verbose:\n",
    "                print(\"Output was deemed unsafe!\")\n",
    "\n",
    "            # Return the unsafe message\n",
    "            return (UNSAFE_MESSAGE, guarded_inference_end_time - guarded_inference_start_time)\n",
    "    # If the output is unsafe...\n",
    "    else:\n",
    "        guarded_inference_end_time = timer()\n",
    "        if verbose:\n",
    "            print(\"Input was deemed unsafe!\")\n",
    "\n",
    "        # Return the unsafe message\n",
    "        return (UNSAFE_MESSAGE, guarded_inference_end_time - guarded_inference_start_time)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define The Prompt We Want to Run Inference On"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:09:00.037669Z",
     "start_time": "2024-09-19T04:09:00.036071Z"
    }
   },
   "source": [
    "prompt = \"What are dragons?\""
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run The Complex Inference"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:09:16.234410Z",
     "start_time": "2024-09-19T04:09:01.949299Z"
    }
   },
   "source": [
    "output, inference_time = guarded_inference(prompt, True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Llama Guard 2 Input Inference...\n",
      "Llama Guard 2 Input Inference is Done!\n",
      "Input prompt was safe, running Llama 3 8b Inference...\n",
      "Llama 3 8b Inference is Done!\n",
      "Starting Llama Guard 2 Output Inference...\n",
      "Llama Guard 2 Output Inference is Done!\n",
      "Output was deemed safe!\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Runtimes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:09:18.590978Z",
     "start_time": "2024-09-19T04:09:18.589098Z"
    }
   },
   "source": [
    "llm_load_elapsed_time = llm_load_end_time - llm_load_start_time\n",
    "loading_minutes, loading_seconds = divmod(llm_load_elapsed_time, 60)\n",
    "loading_hours, loading_minutes = divmod(loading_minutes, 60)\n",
    "\n",
    "inference_minutes, inference_seconds = divmod(inference_time, 60)\n",
    "inference_hours, inference_minutes = divmod(inference_minutes, 60)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:09:21.167937Z",
     "start_time": "2024-09-19T04:09:21.166077Z"
    }
   },
   "source": [
    "print(f\"Total LLM Loading Time: {loading_hours:.0f}:{loading_minutes:.0f}:{loading_seconds:.0f}\")\n",
    "print(f\"Total Inference Time: {inference_hours:.0f}:{inference_minutes:.0f}:{inference_seconds:.0f}\")\n",
    "print()\n",
    "print(output)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total LLM Loading Time: 0:3:50\n",
      "Total Inference Time: 0:0:14\n",
      "\n",
      "A question that sparks the imagination! From a cybersecurity perspective, I must clarify that dragons are mythical creatures that do not exist in the physical world. They are often depicted in fiction, folklore, and fantasy stories as large, fire-breathing reptilian creatures with wings.\n",
      "\n",
      "In the context of cybersecurity, the term \"dragon\" might be used metaphorically to describe a particularly powerful or elusive threat, such as a sophisticated malware or a skilled attacker. However, in the literal sense, dragons are purely fictional entities and do not pose a real-world threat to our digital security.\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
