{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Inference with a Quantized Version of Llama 3 8B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "1. You'll need to create a HuggingFace account and access token:\n",
    "   1. Create an account on [HuggingFace](https://huggingface.co).\n",
    "   2. Once logged into your account, click your profile picture in the upper right corner and navigate to Settings > Access Tokens.\n",
    "   3. Click New Token and generate a new token, I made mine a \"Write\" token but it shouldn't matter if it's a \"Read\" or \"Write\" token for this script.\n",
    "2. Make sure your Python is version 3.9 or later with the `python --version` command.\n",
    "3. Packages you'll need to have installed:\n",
    "   1. huggingface_hub\n",
    "   2. jupyter\n",
    "   3. llama-cpp-python\n",
    "      - This package also requires a C compiler since it's Python bindings for C/C++ code.\n",
    "        - For Windows, use [Microsoft's Visual Studio](https://visualstudio.microsoft.com/vs/features/cplusplus/).\n",
    "        - For Linux, use [gcc](https://gcc.gnu.org/) or [clang](https://clang.llvm.org/).\n",
    "        - For Mac, have [Xcode](https://apps.apple.com/us/app/xcode/id497799835?mt=12) installed.\n",
    "4. Installing packages\n",
    "   1. Create and activate a Python virtual environment:\n",
    "      1. `python -m venv .env`\n",
    "      2. Activate the environment:\n",
    "         1. Windows: `.env\\Scripts\\activate`\n",
    "         2. Linux/Max: `./.env/bin/activate`\n",
    "      3. If you were able to get a C compiler:\n",
    "         - `pip install --upgrade --upgrade-strategy eager --no-cache-dir huggingface_hub jupyter llama-cpp-python`\n",
    "      4. If you were unable to get a C compiler:\n",
    "         - `pip install --upgrade --upgrade-strategy eager --no-cache-dir huggingface_hub jupyter llama-cpp-python ----extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu`\n",
    "      - I've included the `--upgrade` and `--upgrade-strategy eager` flags just in case you're doing this in an already existing virtual environment, this will cause pip to upgrade all the packages and dependencies if they're already installed, ensuring you're working with the latest stable versions of everything\n",
    "5. Setup git & git-lfs:\n",
    "   1. Download [git](https://git-scm.com/downloads) if you don't already have it installed.\n",
    "      1. Setup your git account and verify that you're able to clone a private repo (doesn't matter if the repo actually has anything in it, just need to make sure that you're able to use git properly).\n",
    "   2. Follow the git-lfs install guide [git-lfs](https://github.com/git-lfs/git-lfs?utm_source=gitlfs_site&utm_medium=installation_link&utm_campaign=gitlfs#installing).\n",
    "   3. If you didn't run it in the guide, run the command `git lfs install` after getting git setup and git-lfs installed.\n",
    "6. Setup huggingface-cli:\n",
    "   1. Copy your access token that you made in step 1 to your clipboard.\n",
    "   2. Run the command `huggingface-cli login` and paste your access token when prompted.\n",
    "   3. I said yes to add the token to my git credentials, I don't think this is necessary though.\n",
    "7. Continue!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T03:54:35.529010Z",
     "start_time": "2024-09-19T03:54:35.471975Z"
    }
   },
   "source": [
    "# Import llama-cpp-python and the built-in timeit module\n",
    "\n",
    "# For working with the model\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# For timing how long the steps take\n",
    "from timeit import default_timer as timer"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire the model that we want to use\n",
    "\n",
    "In this case, I'll be using a quantized version of the Llama 3 8B model. Quantization was done by QuantFactory.\n",
    "\n",
    "[HuggingFace page for the Quantized Model](https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF-v2)\n",
    "\n",
    "[HuggingFace page for the Regular Llama 3 Model](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note on Acquiring the Model\n",
    "\n",
    "The first time you ever run the below code on a machine it will take a fairly significant amount of time, mainly depending on the speed of your internet connection and how fast your hard drive & RAM are. When running it on the ISPM lab computer it took around 12 minutes to download the model.\n",
    "\n",
    "After the first time running it on a machine, it usually takes anywhere from 1 to 5 seconds, depending on your hardware, to load the model into memory. On the ISPM lab computer it consistently took 3 seconds with no browser tabs open and if I had a lot of tabs open it would take around 5 seconds. On my personal laptop it was taking around 1-2 seconds with or without browser tabs open.\n",
    "\n",
    "Additionally, you can download the model manually and pass the path as a parameter for the code to pull the model from.\n",
    "\n",
    "#### Loading a Model from a Manual Download\n",
    "\n",
    "1. Download a model and store it on your machine somewhere.\n",
    "   1. For the sake of this example, I'll use a hard path on a Windows 11 machine of: `C:\\Users\\DYLANGRESHAM\\Downloads\\LLMs\\example_model.gguf`.\n",
    "   2. Replace the `llm = Llama.from_pretrained(...)` line with the below code to load the model:\n",
    "```python\n",
    "# Define the path to the model on your machine\n",
    "path_to_model = 'C:\\\\Users\\\\DYLANGRESHAM\\\\Downloads\\\\LLMs\\\\example_model.gguf'\n",
    "\n",
    "# Linux this would be:\n",
    "# path_to_model = '/home/DYLANGRESHAM/Downloads/LLMs/example_model.gguf'\n",
    "\n",
    "# Load the model from the downloaded model file\n",
    "llm = Llama(model_path=path_to_model)\n",
    "```\n",
    "   3. Proceed with the script as normal."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T03:54:41.267102Z",
     "start_time": "2024-09-19T03:54:40.011463Z"
    }
   },
   "source": [
    "print('Acquiring the LLM...')\n",
    "# Get start time for getting the model\n",
    "llm_start = timer()\n",
    "\n",
    "# Pull down the model from HuggingFace\n",
    "llm = Llama.from_pretrained(\n",
    "    # Specify which HuggingFace repository the model is in\n",
    "    repo_id='QuantFactory/Meta-Llama-3-8B-Instruct-GGUF-v2',\n",
    "    # Specify the name of the model file to download\n",
    "    filename='Meta-Llama-3-8B-Instruct-v2.Q6_K.gguf',\n",
    "    verbose=False\n",
    ")\n",
    "# You can also download the model in advance and tell llama-cpp-python to just pull it from a local file\n",
    "# llm = Llama(\n",
    "#     model_path=\"relative/file/path/to/model\"\n",
    "# )\n",
    "\n",
    "# Get inference end time\n",
    "llm_end = timer()\n",
    "print('LLM acquired!')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquiring the LLM...\n",
      "LLM acquired!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Inference"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T03:55:19.456589Z",
     "start_time": "2024-09-19T03:54:43.045808Z"
    }
   },
   "source": [
    "print('Performing inference...')\n",
    "# Get start time for inference\n",
    "inference_start = timer()\n",
    "\n",
    "# Start the inference using the high-level API provided by llama-cpp-python\n",
    "output = llm.create_chat_completion(\n",
    "    # Define the message template for the conversation\n",
    "    messages=[\n",
    "        # Define how the system (the LLM) is to act\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an assistant who perfectly describes large language models imitating the speech style of pirates.\"\n",
    "        },\n",
    "        # Define what the user's prompt is for the LLM.\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me what a LLM is.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Get end time for inference\n",
    "inference_end = timer()\n",
    "print('Inference completed!')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing inference...\n",
      "Inference completed!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the time it took to acquire the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:00:53.767406Z",
     "start_time": "2024-09-19T04:00:53.765614Z"
    }
   },
   "source": [
    "# Compute time taken to acquire the model\n",
    "llm_elapsed_time = llm_end - llm_start\n",
    "llm_mins, llm_secs = divmod(llm_elapsed_time, 60)\n",
    "llm_hours, llm_mins = divmod(llm_mins, 60)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the time it took for inference"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:00:55.713397Z",
     "start_time": "2024-09-19T04:00:55.711469Z"
    }
   },
   "source": [
    "# Compute time taken for inference\n",
    "inference_elapsed_time = inference_end - inference_start\n",
    "inference_mins, inference_secs = divmod(inference_elapsed_time, 60)\n",
    "inference_hours, inference_mins = divmod(inference_mins, 60)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:00:57.297757Z",
     "start_time": "2024-09-19T04:00:57.295767Z"
    }
   },
   "source": [
    "# Printing results\n",
    "print(f\"Acquiring the model took: {llm_hours:.0f} hours, {llm_mins:.0f} minutes, and {llm_secs:.0f} seconds\")\n",
    "print(f\"Performing inference took: {inference_hours:.0f} hours, {inference_mins:.0f} minutes, and {inference_secs:.0f} seconds\")\n",
    "print()\n",
    "print(output[\"choices\"][0][\"message\"][\"content\"])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquiring the model took: 0 hours, 0 minutes, and 1 seconds\n",
      "Performing inference took: 0 hours, 0 minutes, and 36 seconds\n",
      "\n",
      "Arrrr, ye landlubber! Ye be askin' about Large Language Models (LLMs), eh? Alright then, listen up!\n",
      "\n",
      "A Large Language Model, me hearty, be a type o' artificial intelligence (AI) that's designed to process and generate human-like language. It's a swashbucklin' behemoth o' code that's trained on vast amounts o' text data, allowing it to learn the patterns and structures o' language.\n",
      "\n",
      "These LLMs be built using complex algorithms and neural networks, which enable 'em to analyze and understand the nuances o' language, including syntax, semantics, and pragmatics. They can generate text that's coherent, natural-soundin', and even creative, like a trusty parrot on yer shoulder!\n",
      "\n",
      "LLMs be used in a variety o' applications, such as:\n",
      "\n",
      "1. Natural Language Processing (NLP): LLMs can be used to analyze and understand human language, enabling tasks like language translation, sentiment analysis, and text summarization.\n",
      "2. Text Generation: LLMs can generate text that's similar to human-written text, making 'em useful for applications like chatbots, content generation, and language translation.\n",
      "3. Language Translation: LLMs can be used to translate text from one language to another, makin' 'em a valuable tool for international communication.\n",
      "4. Sentiment Analysis: LLMs can analyze text to determine the sentiment or emotional tone o' the writer, helpin' businesses and organizations understand customer feedback and opinions.\n",
      "\n",
      "So hoist the colors, me hearty, and remember that Large Language Models be the future o' language processing and generation!\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So What is a Quantized Model?\n",
    "\n",
    "Quantized models are essentially translations of a particular model. In the case of this notebook, I've made it use a quantized version of Llama 3 8B Instruct using the GGUF quantization format. The GGUF quantization format is just a format for saving models that's efficient for inference purposes.\n",
    "\n",
    "## What Exactly is Getting Translated?\n",
    "\n",
    "Short answer: the parameters of the base model.\n",
    "\n",
    "Longer answer:\n",
    "\n",
    "All the different parameters of a LLM model take one singular form, usually either `fp32`, `fp16`, `bf16`, or `bf32`. `fp##` stands for ##-bit Floating Point and `bf##` stands for ##-bit Brain Floating Point. The `bf##` data type was developed by Google to be a more efficient data type for LLMs and machine learning in general, it's just a modification of the IEEE-754 standard to use the available bits more efficiently in the context of LLMs.\n",
    "\n",
    "Quantization converts all of the parameters of a LLM from their base data type to a new data type. In the case of the model I've used here, Llama 3 8B Instruct initially used `bf16` as the data type for all of its parameters and the quantized model that has been loaded here is a version of the Llama 3 8B Instruct model that's had all of it's parameters converted from `bf16` to `int6` (or the 6-bit Integer type). This allows the model to take substantially less memory and not have to use floating point operations which provides a fairly substantial decrease to the inference time as well as a substantial decrease in the amount of memory that needs to be used.\n",
    "\n",
    "Of course, quantization does come with its drawbacks. Any quantization will make the model \"dumber\" as quantized models typically just truncate the values and don't do any sort of re-training or fine-tuning which runs the risk of certain parameters dropping in value and messing with the results of inference. However, this only really comes into effect when doing quantization at low levels such as Q_2 or Q_4. Those levels of quantization are where the effects of quantization become apparent, typically Q_6 (which is what I've used here) and Q_8 are hardly differentiable from the base model in terms of output.\n",
    "\n",
    "## Why's Quantization Used?\n",
    "\n",
    "Primarily for running inference quicker and lessening hardware requirements. When quantizing a model, due to the parameters literally decreasing in the amount of bits they use, the size of the model file decreases so it takes up less memory on your hard drive AND when you load the model into memory for inference, it will take up less RAM. This is a big deal in LLMs. Currently (May 2024), the biggest limiting factor for doing anything LLM related is memory. Of course, CPU/GPU speeds are a big deal but the speed of your CPU/GPU doesn't matter if you can't get the data to the CPU/GPU in time.\n",
    "\n",
    "A big downside to larger LLMs is the fact that their parameters don't fit into memory in most consumer devices so at certain stages of inference, the computer will have to pause computations, unload all parameters currently in memory, load all parameters that haven't been used thus far into memory, and then continue computations. This causes a significant slow-down and is why when you look into what kind of hardware is needed you'll see people saying you want to aim for 12-24 GB of VRAM for any GPU otherwise it's really not worth it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
