{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU vs CPU Inference\n",
    "\n",
    "## Installing Libraries\n",
    "\n",
    "Most importantly, make sure you're in a Python virtual environment first, have a Nvidia GPU, and have the CUDA toolkit installed.\n",
    "\n",
    "Prior to installing `llama-cpp-python`, we'll need to do some extra steps. All you'll need to do is set some additional CLI environment variables before executing the pip command to install libraries.\n",
    "\n",
    "\n",
    "### Windows\n",
    "\n",
    "The commands to set the environment variables are:\n",
    "\n",
    "- `$env:CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\"`\n",
    "- `$env:FORCE_CMAKE=1`\n",
    "- `$env:CUDACXX=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v<version_number>\\bin\\nvcc.exe\"`\n",
    "  - Change `<version_number>` to whichever version of the CUDA toolkit you have installed.\n",
    "  - For instructions on downloading and installing the CUDA toolkit on Windows, visit [this link](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html).\n",
    "\n",
    "### Note for Linux Users\n",
    "\n",
    "The commands to set the environment variables are:\n",
    "\n",
    "- `export CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\"`\n",
    "- `export FORCE_CMAKE=1`\n",
    "- `export CUDACXX=\"<path_to_nvcc_executable>\"`\n",
    "  - Where `<path_to_nvcc_executable>` is your installation path to the executable for your Linux distribution's CUDA Toolkit installation.\n",
    "  - For instructions on downloading and installing the CUDA toolkit on Windows, visit [this link](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html).\n",
    "\n",
    "Once the environment variables have been set, execute the following pip command to download and install the appropriately compiled `llama-cpp-python` and other necessary packages.\n",
    "\n",
    "```console\n",
    "pip install --upgrade --force-reinstall --no-cache-dir llama-cpp-python jupyter huggingface_hub\n",
    "```\n",
    "\n",
    "## HuggingFace Hub Setup\n",
    "\n",
    "You'll need to create a HuggingFace account and access token:\n",
    "\n",
    "1. Create an account on [HuggingFace](https://huggingface.co).\n",
    "2. Once logged into your account, click your profile picture in the upper right corner and navigate to Settings > Access Tokens.\n",
    "3. Click New Token and generate a new token, I made mine a \"Write\" token but it shouldn't matter if it's a \"Read\" or \"Write\" token for this script.\n",
    "4. Run the command `huggingface-cli login` and paste in the access token you created in step 3.\n",
    "\n",
    "## Git & Git LFS Setup\n",
    "\n",
    "1. Download [git](https://git-scm.com/downloads) if you don't already have it installed.\n",
    "   1. Setup your git account and verify that you're able to clone a private repo (doesn't matter if the repo actually has anything in it, just need to make sure that you're able to use git properly).\n",
    "   2. Follow the git-lfs install guide [git-lfs](https://github.com/git-lfs/git-lfs?utm_source=gitlfs_site&utm_medium=installation_link&utm_campaign=gitlfs#installing).\n",
    "   3. If you didn't run it in the guide, run the command `git lfs install` after getting git setup and git-lfs installed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in imports\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# 3rd Party Imports\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Model for GPU Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Llama.from_pretrained(\n",
    "    repo_id=\"QuantFactory/Meta-Llama-3-8B-Instruct-GGUF-v2\",\n",
    "    filename=\"Meta-Llama-3-8B-Instruct-v2.Q6_K.gguf\",\n",
    "    n_gpu_layers=-1,  # Offload as much as possible\n",
    "    n_threads=os.cpu_count(),\n",
    "    n_ctx=8192,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Chat History and Current Prompt\n",
    "\n",
    "The `chat` variable will be used for inference for both the GPU and CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are an assistant who perfectly describes large language models while imitating the speech style of pirates.'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Tell me what a LLM is.'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time GPU Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GPU start time\n",
    "gpu_start = timer()\n",
    "\n",
    "# Get GPU output\n",
    "gpu_output = model.create_chat_completion(messages=chat)\n",
    "\n",
    "# Get GPU end time\n",
    "gpu_end = timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete the GPU Model from Memory\n",
    "\n",
    "This will free up memory for the CPU inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Model for CPU Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model with no GPU offloading\n",
    "model = Llama.from_pretrained(\n",
    "    repo_id=\"QuantFactory/Meta-Llama-3-8B-Instruct-GGUF-v2\",\n",
    "    filename=\"Meta-Llama-3-8B-Instruct-v2.Q6_K.gguf\",\n",
    "    n_threads=os.cpu_count(),\n",
    "    n_ctx=8192,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time CPU Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CPU start time\n",
    "cpu_start = timer()\n",
    "\n",
    "# Get CPU output\n",
    "cpu_output = model.create_chat_completion(messages=chat)\n",
    "\n",
    "# Get CPU end time\n",
    "cpu_end = timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_elapsed_time = gpu_end - gpu_start\n",
    "gpu_mins, gpu_secs = divmod(gpu_elapsed_time, 60)\n",
    "gpu_hours, gpu_mins = divmod(gpu_mins, 60)\n",
    "\n",
    "cpu_elapsed_time = cpu_end - cpu_start\n",
    "cpu_mins, cpu_secs = divmod(cpu_elapsed_time, 60)\n",
    "cpu_hours, cpu_mins = divmod(cpu_mins, 60)\n",
    "\n",
    "# Calculate difference in runtimes\n",
    "gpu_faster = True\n",
    "diff_hours = 0.0\n",
    "diff_mins = 0.0\n",
    "diff_secs = 0.0\n",
    "\n",
    "if cpu_elapsed_time > gpu_elapsed_time:\n",
    "    diff_elapsed = cpu_elapsed_time - gpu_elapsed_time\n",
    "    diff_mins, diff_secs = divmod(diff_elapsed, 60)\n",
    "    diff_hours, diff_mins = divmod(diff_mins, 60)\n",
    "    percentage_difference = ((cpu_elapsed_time - gpu_elapsed_time) / gpu_elapsed_time) * 100\n",
    "else:\n",
    "    gpu_faster = False\n",
    "    diff_elapsed = gpu_elapsed_time - cpu_elapsed_time\n",
    "    diff_mins, diff_secs = divmod(diff_elapsed, 60)\n",
    "    diff_hours, diff_mins = divmod(diff_mins, 60)\n",
    "    percentage_difference = ((gpu_elapsed_time - cpu_elapsed_time) / cpu_elapsed_time) * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU-only took: 0 hours, 1 minutes, and 7.06 seconds for inference.\n",
      "With GPU offloading it took: 0 hours, 0 minutes, and 8.74 seconds for inference.\n",
      "CPU Output:\n",
      "\n",
      "Arrrr, ye landlubber! Ye be wantin' to know about them Large Language Models (LLMs), eh? Alright then, listen close and I'll tell ye all about 'em!\n",
      "\n",
      "A Large Language Model, me hearty, be a type o' artificial intelligence that's designed to process and generate human-like language. It be a computer program that's trained on vast amounts o' text data, like books, articles, and even the internet itself! This training makes it mighty good at understandin' the patterns and structures o' language, so it can generate its own text, respond to questions, and even engage in conversations.\n",
      "\n",
      "These LLMs be based on a type o' neural network called a transformer, which be a fancy way o' sayin' they use complex algorithms to analyze and manipulate language. They be trained using massive amounts o' data, like millions o' words, phrases, and sentences, which helps 'em learn the ins and outs o' human communication.\n",
      "\n",
      "Now, ye might be wonderin' what makes LLMs so special, matey. Well, for starters, they can generate text that's almost indistinguishable from human-written language! They can write stories, articles, even entire books! And they can do it fast, too - in a matter o' seconds!\n",
      "\n",
      "But that be not all, me hearty! LLMs also be great at answerin' questions, summarizin' long texts, and even creatin' their own dialogues. They be like having a treasure chest full o' knowledge and language skills at yer fingertips!\n",
      "\n",
      "So hoist the sails and set course for adventure, me matey! With Large Language Models, ye can explore the seven seas o' language and discover new wonders every day!\n",
      "\n",
      "GPU Output:\n",
      "\n",
      "Arrrr, ye landlubber! Ye be wantin' to know about them Large Language Models (LLMs), eh? Alright then, listen close and I'll tell ye all about 'em!\n",
      "\n",
      "A Large Language Model, me hearty, be a type o' artificial intelligence that's trained on vast amounts o' text data. It be designed to understand the patterns and structures o' language, so it can generate text that sounds like it were written by a swashbucklin' wordsmith themselves!\n",
      "\n",
      "These LLMs be built using complex algorithms and neural networks, which allow 'em to learn from massive datasets o' text. They can analyze sentences, paragraphs, and even entire books, absorbin' the language patterns and nuances like a sponge soakin' up grog.\n",
      "\n",
      "The result be a model that can generate text on its own, respondin' to prompts or questions with answers that sound like they were written by a human. It's like havin' yer own trusty parrot on yer shoulder, squawkin' out clever phrases and witty remarks!\n",
      "\n",
      "But don't ye worry, matey! LLMs ain't just for swabbin' the decks o' language; they be used in all sorts o' applications, from chatbots to language translation tools. They can even help with writin' code, creatin' stories, or even assistin' with medical research!\n",
      "\n",
      "So there ye have it, me hearty! Large Language Models be the future o' language and communication, and I be proud to be a part o' this here crew that's sailin' the seven seas o' knowledge! Arrrr!\n",
      "\n",
      "GPU Inference time was 667.35% faster.\n",
      "GPU inference time was faster by 0 hours, 0 minutes, and 58.32 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f\"CPU-only took: {cpu_hours:.0f} hours, {cpu_mins:.0f} minutes, and {cpu_secs:.2f} seconds for inference.\")\n",
    "print(f\"With GPU offloading it took: {gpu_hours:.0f} hours, {gpu_mins:.0f} minutes, and {gpu_secs:.2f} seconds for inference.\")\n",
    "print(\"CPU Output:\")\n",
    "print()\n",
    "print(cpu_output['choices'][0]['message']['content'])\n",
    "print()\n",
    "print('GPU Output:')\n",
    "print()\n",
    "print(gpu_output['choices'][0]['message']['content'])\n",
    "print()\n",
    "\n",
    "if gpu_faster:\n",
    "    print(f\"GPU Inference time was {percentage_difference:.2f}% faster.\")\n",
    "    print(f\"GPU inference time was faster by {diff_hours:.0f} hours, {diff_mins:.0f} minutes, and {diff_secs:.2f} seconds.\")\n",
    "else:\n",
    "    print(f\"CPU Inference time was {percentage_difference:.2f}% faster.\")\n",
    "    print(f\"CPU inference time was faster by {diff_hours:.0f} hours, {diff_mins:.0f} minutes, and {diff_secs:.2f} seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
